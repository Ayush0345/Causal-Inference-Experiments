# -*- coding: utf-8 -*-
"""6332-Q2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w4MAj82eo6D7bBVzjxTw_VZhcEpbNJrU

Q2. (a) Descriptive Statistics of the Sample
"""

import numpy as np
import pandas as pd
import statsmodels.api as sm
import statsmodels.formula.api as smf
from itertools import combinations
import plotnine as p
import matplotlib.pyplot as plt
from sklearn import linear_model
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from causallib.datasets import load_nhefs
from causallib.estimation import IPW
from causallib.evaluation import evaluate

data = '/content/catholic.csv'
catholic = pd.read_csv(data)

summary = catholic.describe()
print(summary)

"""(b) Estimating the OLS model:"""

model = sm.OLS.from_formula('math12 ~ catholic', data=catholic).fit()
print(model.summary())

"""Estimate of Î² = 3.8949

The estimate is biased because of K dimensions in the vector of covariates X, leading to the curse of dimensionality. Moreover, it makes measuring the Euclidean distance to match the covariates harder as well. As the distance between matched covariates moves away from 0, it creates a strong bias in the estimates. Therefore, we need to correct the endogeneity problem generated by the model through propensity score matching.

(c) Set of observables related to 'Catholic school attendance' are 'Female school attendance, Race, Parents' marriage, Family income, Parents' education'.

(d) Estimating propensity score using a logit model

First, measuring the Euclidean distance to make sure that the distance between the covariates is appropriate for matching.

Them estimating the logit model to generate the propensity score.
"""

mean1 = catholic[catholic.catholic == 1].catholic.mean()
mean0 = catholic[catholic.catholic == 0].catholic.mean()

ate = np.unique(mean1-mean0)[0]
print("The experimental ATE estimate is {:.2f}".format(ate))

!pip install PsmPy

from psmpy import PsmPy
from psmpy.functions import cohenD
from psmpy.plotting import *
sns.set(rc={'figure.figsize':(10,8)}, font_scale = 1.3)

psm = PsmPy(catholic, treatment='catholic', indx='i', exclude = [])
psm.logistic_ps(balance = True)
psm.predicted_data

"""(e) Using the propensity score as the vector of covariates to get the causal inference of catholic school attendance on 12th grade math scores. (ii). Nearest neighbor matching using propensity score matching."""

psm.knn_matched_12n(matcher='propensity_score', how_many=3)
psm.plot_match(Title='Matched treatment', Ylabel='Indicators', Xlabel= 'Propensity Score', colors=['#E69F00', 'b'] ,save=True)
psm.effect_size_plot(title='Causal Effect of Catholic School Attendance of 12th Grade Math Scores', before_color='r', after_color='g', save=True)

"""(i) Inverse Propensity Weighting"""

ipw = IPW(LogisticRegression(solver='liblinear'))
results = evaluate(ipw, catholic, catholic['catholic'], catholic['math12'], cv="auto")
fig, ax = plt.subplots()
results.plot_covariate_balance(kind="love", ax=ax, thresh=0.1)

"""The Love plot calculates the standardized mean difference between groups for each covariate before and after weighting. Therefore, after weighting the difference for all covariates will be smaller than some threshold, usually 0.1. This result indicates that the weighting successfully created a pseudo population where the covariates are equally distributed between the two groups. (Zohar, 2022. Hands on Inverse-Propensity-Weighting in Python using causallib. Towards Data Science.)"""